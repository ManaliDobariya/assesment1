{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c02bd7",
   "metadata": {},
   "source": [
    "## Module 9 : Natural Language Processing [NLP] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04530df",
   "metadata": {},
   "source": [
    "### 1. What do you understand by Natural Language Processing? "
   ]
  },
  {
   "cell_type": "raw",
   "id": "8cc22ab6",
   "metadata": {},
   "source": [
    "NLP stands for natural language processing which is a part of computer science, human language, and artificial inteligence.\n",
    "It is the technology that is used by machine to understand, analyse, manipulate and interpret human's language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6f36bf",
   "metadata": {},
   "source": [
    "### 2. What are the steps involved in solving an NLP problem? \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "982b9568",
   "metadata": {},
   "source": [
    "Step 1: Gather your data\n",
    "Step 2: Clean your data\n",
    "Step 3: Find a good data representation\n",
    "Step 4: Classification\n",
    "Step 5: Inspection\n",
    "Step 6: Accounting for vocabulary structure\n",
    "Step 7: Leveraging semantics\n",
    "Step 8: Leveraging syntax using end-to-end approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10230d",
   "metadata": {},
   "source": [
    "### 3. What is an ensemble method in NLP? With Example. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef908d61",
   "metadata": {},
   "source": [
    "Both bagging and boosting are ensemble learning techniques used to improve the performance of models by combining multiple weak \n",
    "learners.\n",
    "1. Bagging:\n",
    "Bagging involves training multiple instances of the same learning algorithm on different subsets of the training data and then \n",
    "combining their predictions. The subsets are typically sampled with replacement, and each model in the ensemble is trained \n",
    "independently. The final prediction is usually determined by averaging the predictions of all the models\n",
    "or by a majority vote (for classification tasks).\n",
    "\n",
    "Example:\n",
    "Let's consider a classification problem where we want to classify images as either cats or dogs. \n",
    "We have a dataset of 1000 images with labels. To apply bagging, we would:\n",
    "\n",
    "Randomly sample subsets of the training data with replacement. For instance, we might create 10 subsets, each containing 800 \n",
    "randomly selected images from the original dataset.\n",
    "Train a classification model (e.g., decision tree, random forest) on each subset independently.\n",
    "When making predictions on a new image, we would pass it through each of the trained models and aggregate their predictions. \n",
    "For classification, we could take the majority vote of all models to determine the final class label.\n",
    "\n",
    "2.Boosting:\n",
    "Boosting is an ensemble learning technique that iteratively improves the performance of a weak learner by focusing on examples \n",
    "that were previously misclassified. Unlike bagging, where each model in the ensemble is trained independently, boosting trains \n",
    "models sequentially, and each subsequent model focuses on the mistakes made by the previous ones. Boosting aims to reduce bias \n",
    "and variance by combining multiple weak learners into a strong learner.\n",
    "\n",
    "Example:\n",
    "Let's continue with the previous example of classifying images as cats or dogs. To apply boosting, we would:\n",
    "\n",
    "Train a weak learner on the entire training dataset.\n",
    "Identify the instances that the weak learner misclassified.\n",
    "Increase the importance of the misclassified instances and decrease the importance of correctly classified instances.\n",
    "Train another weak learner on the modified dataset, giving more weight to the misclassified instances.\n",
    "Repeat steps 2-4 for a predefined number of iterations.\n",
    "Combine the predictions of all weak learners using a weighted sum to obtain the final prediction.\n",
    "\n",
    "Boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting), each with \n",
    "its own variations and hyperparameters. These algorithms iteratively build a strong learner by focusing on difficult examples \n",
    "and adjusting the weights of training instances.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
